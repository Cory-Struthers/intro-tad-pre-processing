---
title: "Pre-processing text"
subtitle: "Introduction to Text as Data"
author: "Amber Boydstun & Cory Struthers"
date: "April 27-29, 2023"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    code_folding: show
    highlight: tango
    theme: united
    toc: yes
    df_print: paged
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = "/Users/cs86487/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules/data/")
```

### Introduction

After data has been gathered or collected, we load the text data with the corresponding metadata. We then review patterns. Patterns in the text can provide useful context about the types of analysis choices that may be (in)appropriate based on the data generation process (DGP). 

In this module, we'll need the following packages:

``` {r, results = 'hide', message = FALSE}

# Load packages
require(tidyverse)
require(readxl)
require(quanteda)
library(quanteda.textstats)

# Set working directory
setwd("/Users/cs86487/Dropbox/text-as-data-JUST-CORY-AND-AMBER/modules/data/")
getwd() # view working directory

```


First we upload the raw data, which sometimes includes metadata about the text itself (e.g., the speaker of the text, source of the text).

``` {r, message = FALSE}

# Load data and with doc_id_keep
news_data = read_xlsx("sample_news_1995-2017_docid.xlsx") 
tweet_data = read.csv("sample_immigration_tweets_2013-2017_docid.csv") 
bills_data = readRDS("bills_sponsors_sample_2019-2020_docid.rds") 

# Examine metadata
names(news_data)
names(tweet_data)
names(bills_data)

# Read text
news_data$text[1]
bills_data$text[2] 
tweet_data$text[3] 

```

\

---

**Question 1: Review a sample of each text dataframe. Describe differences you observe between the sources, both format and content. What patterns do you notice in each data source?**

---

\

### Manipulate raw text

Oftentimes, we identify a pattern in a body of text from the DGP that could lead to false conclusions. Say that we want to create a network linking members of Congress to other speakers they address in their legislative speeches to uncover informal coalitions among members. Creating this network would involve using computational text methods to identify members of Congress that come up in the text. But members of Congress often yield their time to colleagues at the end of their speech -- a procedure that may have little to do with coalition building. Leaving the statement where time is yielded would introduce considerable noise and error into the analysis.  

One way we can handle such problematic patterns is editing the text before turning it into a corpus. We can use basic functions in R (like gsub and grep) and packages like stringi and stringr to edit text, relying heavily on regular expressions (RegEx) to edit out problematic text. Regular expressions are useful because they can identify a sequence of characters that specifies a search pattern in text. Once that search pattern has been identified, we can use commands in R to make changes to the text. Like many programming opportunities, RegEx can be frustrating to work with but has incredible potential. Websites like *https://regex101.com/*, online R resources for data science -- [Robert Peng's explanation of regular expressions in R, for example](https://bookdown.org/rdpeng/rprogdatascience/regular-expressions.html) or [Dave Child's cheat sheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/pdf_bw/), and Stack Overflow are your friends when you need to edit text.  

It's important to note that in many situations, patterns that stem from the DGP will _not_ cause systematic bias in a text analysis. In the example below, we demonstrate one way to use RegEx to remove the text that comes before the phrase "legislative counsel's digestab" in the bills data, including the title of the bill and other preamble. Whether these terms and preamble are problematic depends heavily on the research question. Whereas the phrase would likely cause few inferential challenges in a topic model used to measure issue emphases in legislative bills, it might be problematic in a word frequency count used to measure the distribution of venues legislators pay attention to in policymaking.


```{r, message = FALSE}

# Using regular expressions to drop common phrase and preamble in bill text
bills_data$text[100]
bills_data$text_processed = gsub(".*legislative counsel's digest", "", bills_data$text)
  # Create new column for edited text to preserve original data

# Check a sample of the processed text 
bills_data$text_processed[100]

```
  

### Create a corpus

After collecting data and removing problematic patterns, it's time to create a corpus. A corpus object saves text (character strings) and variables about the documents (metadata). A corpus saves text strings in a format that can be analyzed with text-as-data tools and pairs text with document-level variables (metadata) that tell us something about the text. Almost always, we're interested in understanding differences in the text according to one or more document-level variables. These are called "docvars" in Quanteda.

We can create a toy corpus from a character vector.

```{r, message = FALSE}

# Character vector with names
animals = c("Amber loves animals", "Animals love Amber", 
            "Which is Amber's favorite animal?", 
            "Which Amber is an animal's favorite?")

# Create corpus 
anim_corp = corpus(animals)
print(anim_corp)

```

More often, we create a corpus from a dataframe that includes text and docvars. Every corpus includes a "doc_id", which is easier assigned by default in the format "text#" or by the researcher using the `docid_field` option in the `corpus` function. 

```{r, message = FALSE}

# Now that raw is processed, drop old text field
bills_data$text = NULL

# Specify the correct text field 
bills_corp = corpus(bills_data, text_field = "text_processed")
print(bills_corp)
  # Confirming: 1,000 documents with 8 docvars

# Remove original dataframe
rm(bills_data)

# Creating two other corpora
news_corp = corpus(news_data, text_field = "text")
tweet_corp = corpus(tweet_data, text_field = "text")

```

We can extract metadata using `docvars`, which stores the metadata in the columns of the text file we transformed to a corpus. This is useful in a workflow where the corpus has been saved as an RDS file and we no longer rely on the original dataframe. We can access docvars using the `$` operator, like we would for a dataframe. 

We can create docvars in the corpus as well.

```{r, message = FALSE}

# View unique sessions in bills corpus 
unique(bills_corp$session)

# Create a docvar 
bills_corp$sessionStartYear = as.numeric(substr(bills_corp$session, 1, 4))
head(bills_corp$sessionStartYear, 10)

# View sample of docvars
head(docvars(bills_corp))

# Make a dataframe of docvars to investigate distributions
bills_docvars_df = docvars(bills_corp)
count(bills_docvars_df, session)

```

There are a number of ways to transform the corpus. First, we can subset the corpus based on docvars. Second, we can also reshape the corpus into different units: documents, paragraphs, and sentences. Third, we can use extract segments of text based on patterns, including regular expressions, similar to actions we took we pre-processed the raw text. Segmenting is often needed to split a document based on a speaker. This is a really powerful feature of Quanteda because it does the string splitting for you. All docvars are retained during these transformation.


```{r, message = FALSE}

# Subset to original tweets only
tweet_corp_noRT = corpus_subset(tweet_corp, is_retweet == 0)
ndoc(tweet_corp_noRT)

# Subset to Washington Post &  NY Times only
news_corp_wp_nyt = corpus_subset(news_corp, Source %in% c("new york times", "washington post"))
ndoc(news_corp_wp_nyt)

# Reshape to sentences
bills_corp_sent = corpus_reshape(bills_corp, to = "sentences")
ndoc(bills_corp_sent)

# Speaker example
speeches_corp = corpus("Mrs. Taro from Alabama: I yield my time.
                        Ms. Jeffers from Oregon: Thank you Chairman.
                        Mr. Kip from Minnesota: We must act on our borders now.")
speakers_corp = corpus_segment(speeches_corp, pattern = "\\b[A-Z].+\\s[A-Z][a-z]+:", valuetype = "regex")
head(speakers_corp)

# Add speaker name to docvars
cbind(docvars(speakers_corp), text = as.character(speakers_corp))

# Segment at tag
tweet_corp_tags = corpus_segment(tweet_corp, pattern = "@\\s*(.*?)\\s*:", valuetype = "regex", 
                            extract_pattern = TRUE, pattern_position = "before")
ndoc(tweet_corp_tags)
head(tweet_corp_tags)

# Add tagged to docvars 
cbind(docvars(tweet_corp_tags), text = as.character(tweet_corp_tags))

```


The corpora are now ready for analysis. Like an R dataframe, we can save the corpora as an RDS object to avoid repeating these steps in a workflow.

```{r, message = FALSE}

saveRDS(news_corp, "news_corp.RDS")
saveRDS(tweet_corp, "tweet_corp.RDS")
saveRDS(bills_corp, "bills_corp.RDS")

```

\

### Tokenize corpus and reduce complexity

Tokenizing a corpus breaks down a document into discrete words. In English, distinct words are separated using white space, allowing us an easy way of breaking the words apart from one another. From a coding perspective, tokenization is simply splitting the text up by white space. One useful way to tokenize documents in R is to use the `tokens()` from `quanteda package`.Reducing complexity in this way will help us with computation and make downstream analysis parsimonious. Here, we introduce three common steps to reduce the complexity of our text data.

Many text analysis methods require tokenization of the corpus. Tokenization breaks a corpus into vectors of words, n-grams, or sentences. Tokenization is one of the most critical steps in pre-processing because the researcher makes a number of decisions that will affect analysis outcomes. These decisions include:

* **Removing punctuation, URLs, symbols, numbers**: For many research questions, researchers are often not interested in how many commas, apostrophes, periods, symbols, URLS, or numbers appear within the text, and thus choose to delete them.

* **Lowercasing**: Replace all capital letters with lowercase letters. The idea is that the word "This" is no different than "this" for most projects and research questions.

* **Removing stopwords**: Stop words are common words used across documents that do not give much information about the task at hand. In English, common words such as "and", "the", and "that" may be removed from the documents to reduce the size and complexity of the feature set.

* **Generating n-grams**: We can incorporate elements of positional analysis by identifying *n-grams* or multi-word expressions (phrases). Analysts can identify specific n-grams to capture in a tokens document or convert the whole tokens object to n-grams (n-grams of 2 and 3 are most common). N-grams are important for BOW approaches that would otherwise ignore the conceptual link between compound words like "death" and "penalty". Collocation analysis can be used to identify common n-grams in the text.

* **Stemming**: Many words and phrases are conjugated but hold the same meaning. For example, in most contexts, the word "immigrant" has the same meaning as "immigrants" or "immigration". Asterisks match zero or more characters in `quanteda`. For the concept "immigrant", for example, we can use "immig*". We can also use `tokens_wordstem` in `quanteda` to stem all words.

* **Maintaining document length**: Sometimes we need to retain information on terms (tokens) we remove, for example when applying methods that utilize information on word ordering. `quanteda` offers the `padding` option, which is defaulted to `FALSE`. When true, an empty string will remain where removed tokens previously existed.

`quanteda` provides all the functions we need to reduce the complexity of our text data following the above steps.

```{r, message = FALSE}

# Tokenize with no adjustments
toks_news = tokens(news_corp)
print(toks_news[1:2])

# Tokenize removing punctuation, symbols, and numbers
toks_news = tokens(news_corp, 
                   remove_url = TRUE, 
                   remove_punct = TRUE, 
                   remove_symbols = TRUE, 
                   remove_numbers = TRUE)
print(toks_news[1:2]) # all the words in each review are broken down into discrete words

# Use token_select to further reduce types of tokens *after initial tokenization*
toks_news_subs = tokens_select(toks_news, pattern = stopwords("en"), 
                               selection = "remove",
                               padding = FALSE)  # TRUE keeps document length (necessary for positional analysis) 
print(toks_news_subs[1:2])

# The same task can be accomplished with tokens_remove
toks_news_subs = tokens_remove(toks_news, stopwords("english"))
print(toks_news_subs[1:2])

# Lowercase
toks_news_subs = tokens_tolower(toks_news_subs)
print(toks_news_subs[1:2])

# Stem words
toks_news_subs = tokens_wordstem(toks_news_subs)

# Alternatively, pipe functions in one ordered step
toks_news_subs = tokens(news_corp, 
                        remove_url = TRUE, 
                        remove_punct = TRUE, 
                        remove_symbols = TRUE, 
                        remove_numbers = TRUE) %>% 
    tokens_select(pattern = stopwords("en"), selection = "remove") %>% # option to "keep"
    tokens_tolower() %>%
    tokens_wordstem()
print(toks_news_subs[1:2])

```

\

---

**Question 2. When might lowercasing or stemming a corpus be problematic?**

---

\ 

More often than not, words only meaningfully represent topics, sentiment, or some other latent concept when put together. For example, "death" on its own has an entirely different meaning than "death penalty". These phrases are called "n-grams" or "compound tokens" and must be included in the process of tokenization or will otherwise not be captured in the subsequent steps of analysis.

There are two approaches to dealing with phrases. One option is using `tokens_ngrams()`, where the user specifies the number of words that should be included in each token. This approach transforms the entire corpus into phrases. While both efficient and useful when the user is uncertain about the scope of meaningful phrases, the approach also returns a very large tokens object without necessarily adding much information. Alternatively, the user can apply `tokens_compund` to capture n-grams more selectively. 

```{r, results = 'hide', message = FALSE}

# Convert tokens to n-grams using first approach
toks_news_ngrams = tokens_ngrams(toks_news_subs, n = 2:3) # specifying 2 and 3 n-grams
head(toks_news_ngrams[[1]], 15)
head(toks_news_ngrams[[2]], 15)

# Use token_compound to add n-grams selectively
toks_news_comp = tokens_compound(toks_news_subs, 
                                 pattern = phrase(c("climate *", "death penalty", 
                                                    "border fence", "historical sites")))
print(toks_news_comp[1:2])
```

`quanteda` offers a useful function for exploring the context around key words of interest with the function `kwic`. You can specify words or phrases and use the `window` option to specify how many words should appear on either side of the key word or phrase. Note that the results include multiple rows for the same text, each time the key word or phrase is repeated.

```{r, results = 'hide', message = FALSE}

# Use kwic to examine output - remember to use the underscore! 
kw_news = kwic(toks_news_comp, pattern = c("immigration", "death"))
head(kw_news, 10)

kw_news_phrases = kwic(toks_news_comp, pattern = phrase(c("immigration reform", "death penalty")))
head(kw_news_phrases, 10)

# you can also use underscore in "pattern" to produce phrases when text includes compound tokens
kw_news_phrases = kwic(toks_news_comp, pattern = c("immigration_reform", "death_penalty"))
head(kw_news_phrases, 10)

```

Finally, how might we identify compound tokens--when we don't know what they are? Collocation analysis, which identifies words that are commonly seen together in a text, is an efficient way to locate common multi-word expressions. Sometimes we are familiar enough with the text that we can identify collocations by entering keywords, as we've done above. More often -- and particularly in large texts -- we need collocation analysis to identify words that commonly co-occur with other words. Analysts can search for collocations with certain attributes, such as proper nouns (with capitalized words), or search for collocations more freely and across the whole text. `textstat_collocations` can be used on tokens or corpus objects.

```{r, message = FALSE}

# Find capitalized collocations - note, we've returned to token object that hasn't been lowercased! 
news_coll_cap = tokens_select(toks_news, pattern = "^[A-Z]", 
                                valuetype = "regex", 
                                case_insensitive = FALSE) %>% 
              textstat_collocations(min_count = 10, size = 2:3) # term frequency, size of n-gram
head(news_coll_cap, 20)

# Find all collocations 
news_corp_coll = textstat_collocations(toks_news, size = 2:3, min_count = 10)
news_corp_coll

```

\

---

**Question 3. Why might collocation analysis be problematic on a tokens object? How might we improve the meaning of the collocation analysis above?**

---

\


```{r, message = FALSE}

# Remove stopwords
toks_news_no_stop = tokens_select(toks_news, pattern = stopwords("en"), 
                               selection = "remove", padding=TRUE) %>% # add padding to preserve sentence structure
                    tokens_tolower()
print(toks_news_no_stop[1:2])

# Let's again
news_corp_coll_no_stop = textstat_collocations(toks_news_no_stop, size = 2:3, min_count = 10)
news_corp_coll_no_stop

```


Finally, we can add collocations to the tokens object, and remove unnecessary padding we preserved in the initial core tokens object.

```{r, message = FALSE}

# Add to tokens document
news_toks_all =  tokens_compound(toks_news_no_stop, news_corp_coll_no_stop, concatenator = " ") %>%
    tokens_select(padding=FALSE) %>% # remove padding
    tokens_wordstem # stem words after adding collocations
tail(news_toks_all)

```


\

---

**Question 3 (BREAKOUT). Write a regular expression to drop the retweet (RT) information in the text column of tweet_data.**

---

\


**Question 4 (BREAKOUT). Make a corpus for the tweet data. Subset the corpus to years 2016 and 2017. Then tokenize the tweet data using options you feel are most appropriate for the text. **

---

\






